# 쿠버네티스 오브젝트 소개

지금까지 파드를 안정적으로 사용하는 방법을 배우며 파드를 관리하는 여러 가지 기능이 포함된 디플로이먼트 오브젝트를 사용해 봤습니다. 디플로이먼트 외에도 용도에 따라 사용할 수 있는 다양한 오브젝트가 이미 정의돼 있습니다.

## 레플리카셋

여러 개의 각기 다른 파드 매니페스트를 사용해 파드의 복제본을 수동으로 만들 수도 있습니다. 하지만 이러한 작업은 오래 걸리고 에러가 발생하기 쉽습니다.
파드 복제 집합을 관리하는 사용자는 이것을 논리적으로 하나의 개체로 정의하고 관리합니다. 
이 개념이 레플리카셋 입니다.

### 파드에서 레플리카셋 찾기

파드가 레플리카셋에 의해 관리되고 있는지, 그렇다면 어떤 레플리카셋에 의해 관리되고 있는지 궁금할 수 있습니다.
이러한 종류의 검색을 가능하게 하고자 레플리카셋 컨트롤러는 자신이 생성하는 모든 파드에 ownerReferences 섹션을 추가합니다.
다음과 같은 명령을 실행해 ownerReferences 섹션을 찾아볼 수 있습니다.

```sh
kubectl get pods <파드이름> -o=jsonpath='{.metadata.ownerReferences[0].name}'
```

### 레플리카셋에 대한 파드 집합 찾기

레플리카셋에서 관리하는 파드의 집합을 확인할 수도 있습니다.
하기와 같이 -l(라벨) 혹은 --selector 플래그를 사용하여 해당 셀렉터와 일치하는 파드를 찾을 수 있습니다.

```sh
kubectl get pods -l app=nginx,version=2

kubectl get pods --selector app=nginx,version=2
```

## 데몬셋

가장 먼저 데몬셋부터 살펴보겠습니다. 데몬셋은 디플로이먼트의 replicas가 노드 수만큼 정해져  있는 형태라고 할 수 있는데, 노드 하나당 파드 한 개만을 생성합니다.

데몬셋은 현재 쿠버네티스 클러스터를 구축하면서 파드간의 통신을 위해 설치한 `Calico` 네트워크 플러그인 과 `kube-proxy`를 생성할 때 사용했습니다. 

이들의 공통점은 노드의 단일 접속 지점으로 노드 외부와 통신하는 것입니다. 따라서 파드가 1개 이상 필요하지 않습니다.

결국 노드를 관리하는 파드라면 데몬셋으로 만드는 게 가장 효율적입니다.


## 컨피그맵

컨피그맵은 이름 그대로 설정을 목적으로 사용하는 오브젝트입니다. 로드밸런서를 사용하기 위해 MetalLB에서는 `컨피그맵`을 사용했습니다. 

인그레스에서는 설정을 위해 오브젝트를 인그레스로 선언했는데 MetalLB는 컨피그맵을 사용한 이유는 인그레스는 오브젝트가 인그레스로 지정되 있지만 MetalLB는 프로젝트 타입으로 정해진 오브젝트가 없어서 범용 설정으로 컨피그맵을 사용했습니다.

## PV와 PVC

앞에서 파드가 언제라도 생성되고 지워진다는 것을 충분히 알았습니다. 쿠버네티스에서 의도적으로 이렇게 구현했습니다. 

하지만 때때로 운영환경에서 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정 값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있습니다.

쿠버네티스는 이런 경우를 위해 아래와 같이 다양한 형태의 볼륨을 제공합니다.

- 임시: emptyDir
- 로컬: host Path, local
- 원격: persistentVolumeClaim, cephfs, cinder, csi, fc, flexVolume, flocker, glusterfs, isci, portworkxVolume, quobyte, rbd, scaleIO, storages, vsphereVolume
- 특수목적: downwardAPI, configMap, secret, azureFile, projected
- 클라우드: awsElasticBlockStore, azureDisk, gcePersistentDisk

다양한 쿠버네티스 볼륨 스토리지 중에 `PV`와 `PVC`의 관계를 이해한다면 다른 볼륨 스토리지도 쉽게 이해할 수 있습니다.

쿠버네티스는 필요할 때 PVC(PersistentVolumeClaim, 지속적으로 사용 가능한 볼륨 요청)을 요청해 사용합니다. PVC를 사용하려면 PV(PersistentVolume, 지속적으로 사용가능한 볼륨)로 볼륨을 선언해야 합니다.

핀트는 PV는 볼륨을 사용할 수 있게 준비하는 단계이고, PVC는 준비된 볼륨에서 일정 공간을 할당받는 것입니다. 비유하면 PV는 요리사(관리자)가 피자를 굽는 것이고, PVC는 손님가 원하는 만큼 피자를 접시에 담아 가져오는 것입니다.

### NFS 볼륨에 PV/PVC를 만들고 파드에 연결하기

PV로 선언할 볼륨을 만들기 위해 NFS 서버를 마스터 노드에 구성합니다. 공유되는 디렉터리는 `/nfs/_shared`로 생성하고, 해당 디렉터리를 NFS로 받아들일 IP 영역은 `192.168.1.0/24`로 정합니다.

옵션을 적용해 /etc/exports에 기록합니다. 옵션에서 rw는 읽기/쓰기, sync는 쓰기 작업 동기화, no_root_squash는 root 계정 사용을 의미합니다. 

```sh
mkdir /nfs_shared # m-k8s NFS 디렉토리 생성

echo '/nfs_shared 192.168.1.0/24(rw,sync,no_root_squash)' >> /etc/exports 

systemctl enable --now nfs # 시스템에 적용 및 NFS 서버 활성화(자동으로 적용됨)
```

공유되는 디렉토리는 `/nfs_shared`로 생성하고, 해당 디렉토리를 NFS로 받아들이는 IP 영역은 192.168.1.0/24로 정합니다.

옵션을 적용해 `/etc/exports`에 기록합니다. 
옵션의 의미는 아래와 같습니다.

- rw: 읽기/쓰기
- sync: 쓰기 작업 동기화
- no_root_sqash: root 계정 사용

이제 PV 오브젝트를 생성합니다.

```sh
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pv.yaml
```

오브젝트 스펙은 아래와 같습니다.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs: # NFS 서버의 연결 위치에 대한 설정
    server: 192.168.1.10
    path: /nfs_share
```

위의 옵션들을 하나씩 살펴보면 `storage`는 실제로 사용하는 용량을 제한하는 것이 아니라 쓸 수 있는 양을 레이블로 붙이는 것과 같습니다. 이는 현재 스토리지가 단순히 `NFS`로 설정돼서 그렇습니다.

`accessModes`는 PV를 어떤 방식으로 사용할지를 정의한 부분입니다. `ReadWriteMany`는 여러개의 노드가 읽고 쓸 수 있도록 마운트 하는 옵션입니다. 

`persistentVolumeReclaimPolicy`는 PVC가 제거됐을 때 PV가 작동하는 방법을 정의한 것 입니다.

여기서는 유지하는 Retain을 사용합니다.

```sh
kubectl get pv
```

PV 상태가 Available임을 확인하면 됩니다.

![스크린샷 2023-04-22 오전 1 30 41](https://user-images.githubusercontent.com/22395934/233688378-ad7daae4-fba8-4936-b60e-9db4ff17e2b3.png)

```sh
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc.yaml
```

해당 오브젝트의 스펙은 아래와 같습니다.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
```

PVC는 PV와 구성이 거의 동일합니다. 차이점은 PV는 사용자가 요청할 볼륨 공간을 관리자가 만들고, PVC는 사용자(개발자)가 볼륨을 요청하는 데 사용한다는 점입니다.

아래 이미지를 보면 PV와 PVC가 연결됐음을 확인할 수 있습니다. 이상한 점은 용량이 설정한 10Mi가 아닌 100Mi라는 것입니다.

사실 용량은 동적으로 PVC를 따로 요청해 생성하는 경우가 아니면 큰 의미가 없습니다.
여기서는 `BOUND`만 확인하면 됩니다. 

![image](https://user-images.githubusercontent.com/22395934/233689138-3251edd3-601e-4492-aea4-74f0481fa409.png)


이제 PVC를 볼륨으로 사용하는 디플로이먼트 오브젝트 스펙을 배포합니다.

```sh
kubectl apply -f ~/_Book_k8sInfra/ch3/3.4.3/nfs-pvc-deploy.yaml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nfs-pvc-deploy
  template:
    metadata:
      labels:
        app: nfs-pvc-deploy
    spec:
      containers:
      - name: audit-trail
        image: sysnet4admin/audit-trail
        volumeMounts:
        - name: nfs-vol
          mountPath: /audit # 볼륨이 마운트될 위치를 지정
      volumes:
      - name: nfs-vol
        persistentVolumeClaim:
          claimName: nfs-pvc 
```

스펙을 보면 맨 마지막 라인의 `claimName`으로 볼륨이 마운트하기 위해 nfs-pvc라는 이름을 사용하고 있습니다.

생성된 파드 중 하나에 임의로 `exec` 명령어로 접속합니다.

```sh
 kubectl exec -it nfs-pvc-deploy-5fd9876c46-g89qx -- /bin/bash
```

![image](https://user-images.githubusercontent.com/22395934/233690621-80eebc3a-2984-4394-bf3e-02d35492ea4e.png)

마스터 노드에서 `audit-trail` 컨테이너 기능을 테스트합니다. 외부에서 디플로이먼트에 접속할 수 있도록 expose로 로드밸런서 서비스를 생성합니다.

```sh
kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer --name=nfs-pvc-deploy-svc --port=80
```

호스트 컴퓨터의 브라우저에서 `192.168.1.12`에 접속해 파드 이름과 IP가 표시되는지 확인합니다.

![image](https://user-images.githubusercontent.com/22395934/233691201-77fdbf72-92c1-4a14-b7ab-8b225e6dd4b6.png)


exec를 통해 접속한 파드에서 `ls /audit` 명령을 실행하여 접속 기록 파일이 남아있는지 확인합니다.

![image](https://user-images.githubusercontent.com/22395934/233691400-a0eac96a-c914-458d-941e-9ec0e241978a.png)


